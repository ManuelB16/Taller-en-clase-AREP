{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente OpenAI inicializado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"Cliente OpenAI inicializado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola, Universo Digital! üååü§ñ ¬°Listos para explorar el vasto cosmos de la informaci√≥n y la creatividad juntos! ¬øQu√© aventura comenzamos hoy?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Escribe un saludo tipo 'Hello World' con un toque creativo de IA.\"\n",
    "resp = client.chat.completions.create(\n",
    " model=\"gpt-4o-mini\",\n",
    " messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    " temperature=0.6\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- temperature=0.1 ---\n",
      "La inteligencia artificial (IA) es una rama de la inform√°tica que se centra en la creaci√≥n de sistemas y programas capaces de realizar tareas que normalmente requieren inteligencia humana. Esto incluye habilidades como el aprendizaje, el razonamiento, la comprensi√≥n del lenguaje natural, la\n",
      "--- temperature=0.5 ---\n",
      "La inteligencia artificial (IA) es un campo de la inform√°tica que se enfoca en crear sistemas y programas capaces de realizar tareas que normalmente requieren inteligencia humana. Esto incluye habilidades como el aprendizaje, el razonamiento, la comprensi√≥n del lenguaje natural, la percepci√≥n\n",
      "--- temperature=0.9 ---\n",
      "La inteligencia artificial (IA) es una rama de la inform√°tica que se centra en la creaci√≥n de sistemas y programas capaces de realizar tareas que normalmente requieren inteligencia humana. Esto incluye el aprendizaje, el razonamiento, la comprensi√≥n del lenguaje natural, la percepci√≥n visual\n"
     ]
    }
   ],
   "source": [
    "for t in [0.1, 0.5, 0.9]:\n",
    " response = client.chat.completions.create(\n",
    " model=\"gpt-4o-mini\",\n",
    " messages=[{\"role\": \"user\", \"content\": \"Describe brevemente qu√© es la IA.\"}],\n",
    " temperature=t,\n",
    " max_tokens=50\n",
    " )\n",
    " print(f\"--- temperature={t} ---\")\n",
    " print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial en la educaci√≥n se refiere al uso de tecnolog√≠as y algoritmos que simulan la capacidad humana para aprender y adaptarse, con el fin de personalizar el aprendizaje y mejorar la ense√±anza. Estas herramientas pueden analizar el rendimiento de los estudiantes, ofrecer recursos personalizados y facilitar la administraci√≥n de tareas educativas, optimizando as√≠ la experiencia de aprendizaje.\n"
     ]
    }
   ],
   "source": [
    "# 5) Primera consulta: respuesta libre\n",
    "prompt = \"Explica en dos frases qu√© es la inteligencia artificial en la educaci√≥n.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7  # m√°s creativo que 0.2, menos que 1.0\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"operation\": \"explanation\",\n",
      "  \"input\": \"¬øQu√© es aprendizaje supervisado?\",\n",
      "  \"output\": \"El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde un modelo es entrenado utilizando un conjunto de datos etiquetados. Esto significa que cada entrada en el conjunto de datos tiene una salida correspondiente conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza com√∫nmente en tareas como clasificaci√≥n y regresi√≥n.\"\n",
      "}\n",
      "\n",
      "Valid JSON ‚Üí {'operation': 'explanation', 'input': '¬øQu√© es aprendizaje supervisado?', 'output': 'El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde un modelo es entrenado utilizando un conjunto de datos etiquetados. Esto significa que cada entrada en el conjunto de datos tiene una salida correspondiente conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza com√∫nmente en tareas como clasificaci√≥n y regresi√≥n.'}\n"
     ]
    }
   ],
   "source": [
    "# 6) Respuesta estructurada en JSON para automatizaci√≥n\n",
    "import json\n",
    "\n",
    "query = \"¬øQu√© es aprendizaje supervisado?\"\n",
    "schema_instruction = (\n",
    "    \"Responde en formato JSON con las claves: operation, input, output. \"\n",
    "    \"operation debe ser 'explanation'; input debe repetir la pregunta; output la explicaci√≥n clara y breve.\"\n",
    ")\n",
    "response_json = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": schema_instruction},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ],\n",
    "    temperature=0.3,       # m√°s determinista para formatos estructurados\n",
    "    max_tokens=300         # suficiente para una explicaci√≥n breve\n",
    ")\n",
    "text = response_json.choices[0].message.content\n",
    "print(text)\n",
    "\n",
    "# (Opcional) intentar cargar como JSON si el modelo devolvi√≥ un objeto v√°lido\n",
    "try:\n",
    "    data = json.loads(text)\n",
    "    print(\"\\nValid JSON ‚Üí\", data)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\nLa salida no es JSON v√°lido literal. Puedes parsearla manualmente o usar validadores/funciones JSON del proveedor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt\": \"Explica brevemente el principio de funcionamiento de un √°rbol de decisi√≥n.\",\n",
      "  \"respuesta\": \"Un √°rbol de decisi√≥n es un modelo de predicci√≥n que utiliza una estructura jer√°rquica para representar decisiones y sus posibles consecuencias. Cada nodo interno del √°rbol representa una pregunta sobre una caracter√≠stica del conjunto de datos, mientras que las ramas representan las respuestas posibles, llevando a nodos hoja que indican la clase o resultado final.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Plantilla reutilizable para el curso\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.3,\n",
    "              max_tokens: int = 400,\n",
    "              system: str | None = None):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(ask_model(\n",
    "    \"Explica brevemente el principio de funcionamiento de un √°rbol de decisi√≥n.\",\n",
    "    temperature=0.4,\n",
    "    system=\"Responde en dos oraciones, tono docente y preciso. Siempre responde JSON con llaves promt y respuesta\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (2.6.1)\n",
      "Collecting langchain\n",
      "  Downloading langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (1.2.1)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain)\n",
      "  Downloading langchain_core-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.0 (from langchain)\n",
      "  Downloading langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading langsmith-0.4.38-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading ormsgpack-1.11.0-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading orjson-3.11.4-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1.0.0,>=0.7.0->langchain-openai)\n",
      "  Downloading regex-2025.10.23-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading langchain-1.0.2-py3-none-any.whl (107 kB)\n",
      "Downloading langchain_core-1.0.1-py3-none-any.whl (467 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
      "Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Downloading langsmith-0.4.38-py3-none-any.whl (397 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_openai-1.0.1-py3-none-any.whl (81 kB)\n",
      "Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl (879 kB)\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 879.4/879.4 kB 7.9 MB/s  0:00:00\n",
      "Downloading orjson-3.11.4-cp311-cp311-win_amd64.whl (131 kB)\n",
      "Downloading ormsgpack-1.11.0-cp311-cp311-win_amd64.whl (112 kB)\n",
      "Downloading regex-2025.10.23-cp311-cp311-win_amd64.whl (277 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl (31 kB)\n",
      "Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl (506 kB)\n",
      "Installing collected packages: zstandard, xxhash, tenacity, regex, ormsgpack, orjson, jsonpatch, tiktoken, requests-toolbelt, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain\n",
      "\n",
      "   ---- -----------------------------------  2/17 [tenacity]\n",
      "   ------- --------------------------------  3/17 [regex]\n",
      "   -------------- -------------------------  6/17 [jsonpatch]\n",
      "   ---------------- -----------------------  7/17 [tiktoken]\n",
      "   ------------------ ---------------------  8/17 [requests-toolbelt]\n",
      "   ------------------ ---------------------  8/17 [requests-toolbelt]\n",
      "   --------------------- ------------------  9/17 [langsmith]\n",
      "   --------------------- ------------------  9/17 [langsmith]\n",
      "   --------------------- ------------------  9/17 [langsmith]\n",
      "   --------------------- ------------------  9/17 [langsmith]\n",
      "   ----------------------- ---------------- 10/17 [langgraph-sdk]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ------------------------- -------------- 11/17 [langchain-core]\n",
      "   ---------------------------- ----------- 12/17 [langgraph-checkpoint]\n",
      "   ---------------------------- ----------- 12/17 [langgraph-checkpoint]\n",
      "   ------------------------------ --------- 13/17 [langchain-openai]\n",
      "   -------------------------------- ------- 14/17 [langgraph-prebuilt]\n",
      "   ----------------------------------- ---- 15/17 [langgraph]\n",
      "   ----------------------------------- ---- 15/17 [langgraph]\n",
      "   ----------------------------------- ---- 15/17 [langgraph]\n",
      "   ----------------------------------- ---- 15/17 [langgraph]\n",
      "   ------------------------------------- -- 16/17 [langchain]\n",
      "   ------------------------------------- -- 16/17 [langchain]\n",
      "   ---------------------------------------- 17/17 [langchain]\n",
      "\n",
      "Successfully installed jsonpatch-1.33 langchain-1.0.2 langchain-core-1.0.1 langchain-openai-1.0.1 langgraph-1.0.1 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.9 langsmith-0.4.38 orjson-3.11.4 ormsgpack-1.11.0 regex-2025.10.23 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.12.0 xxhash-3.6.0 zstandard-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai langchain python-dotenv langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El aprendizaje autom√°tico es una rama de la inteligencia artificial que permite a las computadoras aprender de datos y mejorar su rendimiento en tareas espec√≠ficas sin ser programadas expl√≠citamente para cada una de ellas. Utiliza algoritmos para identificar patrones y hacer predicciones basadas en la informaci√≥n que ha procesado.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the LLM (uses your OpenAI API key from environment)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "# Create a simple prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explica en dos frases el concepto de {tema}.\"\n",
    ")\n",
    "\n",
    "# Combine the components using LCEL (LangChain Expression Language)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run it\n",
    "result = chain.invoke({\"tema\": \"aprendizaje autom√°tico\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Aplicaci√≥n Educativa: \"Exploradores del Conocimiento\"**\n",
      "\n",
      "**Descripci√≥n General:**\n",
      "\"Exploradores del Conocimiento\" es una aplicaci√≥n educativa que utiliza la realidad aumentada para enriquecer el aprendizaje en diversas materias, como historia, ciencias, geograf√≠a y arte. La aplicaci√≥n permite a los estudiantes interactuar con el contenido educativo de manera din√°mica y envolvente, superponiendo informaci√≥n digital sobre elementos del mundo real.\n",
      "\n",
      "**Caracter√≠sticas Principales:**\n",
      "\n",
      "1. **Exploraci√≥n Interactiva:**\n",
      "   - Los estudiantes pueden escanear objetos en su entorno (libros, monumentos, plantas, etc.) con la c√°mara de su dispositivo. La RA superpondr√° informaci√≥n relevante, como datos hist√≥ricos, caracter√≠sticas biol√≥gicas o contextos culturales relacionados con el objeto escaneado.\n",
      "\n",
      "2. **Lecciones Multimedia:**\n",
      "   - La aplicaci√≥n ofrece lecciones en formato de realidad aumentada que incluyen videos, animaciones y gr√°ficos interactivos. Por ejemplo, al escanear un libro de historia, los estudiantes pueden ver una reconstrucci√≥n 3D de un evento hist√≥rico o una entrevista con un personaje relevante.\n",
      "\n",
      "3. **Juegos Educativos:**\n",
      "   - Se incorporan juegos que fomentan el aprendizaje a trav√©s de desaf√≠os y preguntas que los estudiantes deben responder mientras interact√∫an con elementos del entorno. Por ejemplo, un juego de b√∫squeda del tesoro donde deben encontrar ciertos objetos en su entorno y aprender sobre ellos.\n",
      "\n",
      "4. **Colaboraci√≥n y Proyectos:**\n",
      "   - Los estudiantes pueden trabajar en grupos para crear proyectos en RA, donde deben investigar un tema y presentar su trabajo a trav√©s de una experiencia de RA. Esto fomenta la colaboraci√≥n y el aprendizaje activo.\n",
      "\n",
      "5. **Realidad Aumentada en el Aula:**\n",
      "   - Los docentes pueden utilizar la aplicaci√≥n para enriquecer sus lecciones en el aula. Por ejemplo, al ense√±ar sobre el sistema solar, los estudiantes pueden ver planetas en 3D flotando en el aula, interactuando con ellos para aprender sobre sus caracter√≠sticas.\n",
      "\n",
      "6. **Acceso a Recursos:**\n",
      "   - La aplicaci√≥n proporciona acceso a una base de datos de recursos educativos, como art√≠culos, videos y ejercicios, que los estudiantes pueden explorar a trav√©s de la RA.\n",
      "\n",
      "**Beneficios:**\n",
      "- **Aprendizaje Activo:** La interacci√≥n directa con el contenido mejora la retenci√≥n de informaci√≥n y la comprensi√≥n de conceptos complejos.\n",
      "- **Motivaci√≥n y Compromiso:** La experiencia inmersiva y l√∫dica de la RA aumenta la motivaci√≥n de los estudiantes y su inter√©s por aprender.\n",
      "- **Desarrollo de Habilidades:** Fomenta habilidades como la investigaci√≥n, el trabajo en equipo y la creatividad a trav√©s de proyectos colaborativos.\n",
      "\n",
      "**Implementaci√≥n:**\n",
      "\"Exploradores del Conocimiento\" se puede implementar en escuelas y universidades, proporcionando capacitaci√≥n a docentes sobre c√≥mo integrar la RA en sus clases. La aplicaci√≥n tambi√©n puede ser utilizada en casa, permitiendo a los estudiantes continuar su aprendizaje de manera independiente.\n",
      "\n",
      "En resumen, \"Exploradores del Conocimiento\" utiliza la realidad aumentada para transformar la forma en que los estudiantes interact√∫an con el conocimiento, haciendo que el aprendizaje sea m√°s din√°mico, interactivo y relevante para su entorno.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# 1) LLM (usa tu OPENAI_API_KEY en el entorno)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "to_str = StrOutputParser()\n",
    "\n",
    "# 2) Paso 1: explicar brevemente el concepto de {tema}\n",
    "primer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explica brevemente el concepto de {tema}.\"\n",
    ")\n",
    "primer_paso = primer_prompt | llm | to_str\n",
    "# `primer_paso` produce un string, por ejemplo: \"La realidad aumentada es ...\"\n",
    "\n",
    "# 3) Paso 2: proponer una aplicaci√≥n educativa usando la salida del paso 1 como {concepto}\n",
    "segundo_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Prop√≥n una aplicaci√≥n educativa del siguiente concepto: {concepto}.\"\n",
    ")\n",
    "segundo_paso = segundo_prompt | llm | to_str\n",
    "\n",
    "# 4) Encadenar: mapear la entrada {tema} al primer paso, y su salida a {concepto} del segundo\n",
    "cadena_secuencial = {\"concepto\": primer_paso} | segundo_paso\n",
    "\n",
    "# 5) Ejecutar la cadena completa\n",
    "resultado = cadena_secuencial.invoke({\"tema\": \"realidad aumentada\"})\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! ¬øEn qu√© puedo ayudarte hoy con respecto a la inform√°tica?\n",
      "Claro, aqu√≠ hay algunas ideas para introducir la inteligencia artificial (IA) a tus estudiantes:\n",
      "\n",
      "1. **Conceptos B√°sicos**: Comienza con una introducci√≥n a los conceptos fundamentales de la IA, como qu√© es, c√≥mo funciona y sus aplicaciones en la vida cotidiana.\n",
      "\n",
      "2. **Ejemplos Pr√°cticos**: Muestra ejemplos de IA en acci√≥n, como asistentes virtuales (Siri, Alexa), sistemas de recomendaci√≥n (Netflix, Amazon) y chatbots.\n",
      "\n",
      "3. **Proyectos Simples**: Asigna proyectos sencillos, como crear un chatbot b√°sico usando herramientas como Scratch o Python con bibliotecas como NLTK.\n",
      "\n",
      "4. **Herramientas y Plataformas**: Introduce plataformas accesibles para trabajar con IA, como Google Teachable Machine, donde pueden crear modelos de IA sin necesidad de programar.\n",
      "\n",
      "5. **√âtica en IA**: Discute la √©tica de la IA, incluyendo sesgos algor√≠tmicos y el impacto social de la automatizaci√≥n.\n",
      "\n",
      "6. **Recursos Educativos**: Proporciona recursos en l√≠nea, como cursos en Coursera o edX, que ofrezcan una introducci√≥n a la IA.\n",
      "\n",
      "7. **Invitados Especiales**: Considera invitar a expertos en IA para que compartan sus experiencias y respondan preguntas.\n",
      "\n",
      "8. **Competencias Interdisciplinarias**: Relaciona la IA con otras materias, como matem√°ticas (algoritmos), ciencias (modelos predictivos) y arte (generaci√≥n de arte con IA).\n",
      "\n",
      "Recuerda adaptar el contenido al nivel de tus estudiantes y fomentar la curiosidad y el pensamiento cr√≠tico. ¬°Buena suerte!\n",
      "Aqu√≠ tienes algunos ejemplos pr√°cticos que puedes utilizar en clase para ilustrar conceptos de inteligencia artificial:\n",
      "\n",
      "1. **Chatbots**: Utiliza herramientas como Chatbot.com o Dialogflow para que los estudiantes creen su propio chatbot. Pueden programar respuestas a preguntas comunes.\n",
      "\n",
      "2. **Reconocimiento de Im√°genes**: Usa Google Teachable Machine para que los estudiantes entrenen un modelo de IA que reconozca im√°genes o sonidos. Pueden crear un proyecto donde la IA identifique objetos o sonidos.\n",
      "\n",
      "3. **An√°lisis de Sentimientos**: Implementa un proyecto simple en Python utilizando bibliotecas como TextBlob o NLTK para analizar el sentimiento de textos (por ejemplo, comentarios de redes sociales).\n",
      "\n",
      "4. **Juegos de IA**: Introduce juegos simples que utilizan IA, como Tic-Tac-Toe o Ajedrez, y discute c√≥mo se implementan las estrategias de juego.\n",
      "\n",
      "5. **Recomendaciones de Pel√≠culas**: Muestra c√≥mo funcionan los sistemas de recomendaci√≥n. Los estudiantes pueden crear un sistema b√°sico que sugiera pel√≠culas basado en preferencias de g√©nero.\n",
      "\n",
      "6. **Generaci√≥n de Arte**: Utiliza herramientas como DeepArt o DALL-E para que los estudiantes generen im√°genes o arte a partir de descripciones textuales.\n",
      "\n",
      "7. **Asistentes Virtuales**: Muestra c√≥mo funcionan los asistentes virtuales (Siri, Google Assistant). Los estudiantes pueden discutir c√≥mo se procesan las consultas de voz.\n",
      "\n",
      "8. **Automatizaci√≥n de Tareas**: Demuestra c√≥mo se pueden automatizar tareas simples usando scripts de Python, como enviar correos electr√≥nicos o procesar datos.\n",
      "\n",
      "9. **Proyectos de Aprendizaje Autom√°tico**: Si tus estudiantes tienen un nivel m√°s avanzado, pueden trabajar en proyectos de aprendizaje autom√°tico utilizando bibliotecas como TensorFlow o Scikit-learn.\n",
      "\n",
      "10. **Estudio de Casos**: Presenta estudios de caso de empresas que utilizan IA, como Google, Amazon o Tesla, y discute el impacto y los desaf√≠os que enfrentan.\n",
      "\n",
      "Estos ejemplos no solo son pr√°cticos, sino que tambi√©n fomentan la creatividad y el pensamiento cr√≠tico entre los estudiantes. ¬°Espero que te sean √∫tiles!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instalar si hace falta:\n",
    "# %pip install -U langchain langchain-openai\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM (requiere OPENAI_API_KEY en el entorno)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "to_str = StrOutputParser()\n",
    "\n",
    "# Prompt con hueco para el historial\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente educativo claro y conciso.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),      # ‚Üê aqu√≠ va la memoria\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena base\n",
    "chain = prompt | llm | to_str\n",
    "\n",
    "# Memoria simple como lista de mensajes\n",
    "history: list = []\n",
    "\n",
    "def chat(user_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Env√≠a un turno del usuario, usa el historial y actualiza la memoria\n",
    "    con el par (usuario, asistente).\n",
    "    \"\"\"\n",
    "    global history\n",
    "    # Ejecutar la cadena inyectando el historial actual\n",
    "    answer = chain.invoke({\"input\": user_text, \"chat_history\": history})\n",
    "    # Actualizar memoria (guardar los dos mensajes)\n",
    "    history += [HumanMessage(content=user_text), AIMessage(content=answer)]\n",
    "    return answer\n",
    "\n",
    "# --- Ejemplo de uso (tres turnos) ---\n",
    "print(chat(\"Hola, soy un profesor de inform√°tica.\"))\n",
    "print(chat(\"¬øPuedes explicarme c√≥mo introducir IA a mis estudiantes?\"))\n",
    "print(chat(\"¬øQu√© ejemplos pr√°cticos puedo usar en la clase?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manuel.barrera\\downloads\\hello_ai_vscode\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading torch-2.9.0-cp311-cp311-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.6/109.3 MB 7.6 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 3.4/109.3 MB 8.4 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 5.0/109.3 MB 8.4 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 6.8/109.3 MB 8.6 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 8.9/109.3 MB 8.8 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 9.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 9.2 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 9.4 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 17.3/109.3 MB 9.5 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 19.7/109.3 MB 9.7 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 21.8/109.3 MB 9.8 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 24.1/109.3 MB 9.8 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 26.2/109.3 MB 9.9 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 10.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 33.0/109.3 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 35.1/109.3 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 37.2/109.3 MB 10.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 39.3/109.3 MB 10.2 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 41.4/109.3 MB 10.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 43.8/109.3 MB 10.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 45.9/109.3 MB 10.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 48.0/109.3 MB 10.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 50.1/109.3 MB 10.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 52.4/109.3 MB 10.3 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 54.5/109.3 MB 10.3 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 56.6/109.3 MB 10.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 59.0/109.3 MB 10.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 61.1/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 63.4/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 65.5/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 67.6/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 70.0/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 72.1/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 74.2/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 76.5/109.3 MB 10.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 78.6/109.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 80.7/109.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 82.8/109.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 85.2/109.3 MB 10.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 87.6/109.3 MB 10.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.7/109.3 MB 10.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 91.8/109.3 MB 10.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 93.8/109.3 MB 10.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 96.2/109.3 MB 10.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 98.0/109.3 MB 10.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 100.4/109.3 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 102.5/109.3 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 104.9/109.3 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  107.0/109.3 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 MB 10.4 MB/s  0:00:10\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 10.3 MB/s  0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.1/6.3 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.3 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 9.7 MB/s  0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s  0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ---------------------------------------- 4/4 [torch]\n",
      "\n",
      "Successfully installed mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets huggingface_hub tokenizers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9347259ec3204ffe9115ac0cea988b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuel.barrera\\Downloads\\hello_ai_vscode\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manuel.barrera\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed0f28ce0f64cf9a31cb2767390ac7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f8cf486da24c14a450fbf07b15d656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838208620df745798637911d785ef299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.952487051486969}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01c00fb7a474eea962b2c04187416e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuel.barrera\\Downloads\\hello_ai_vscode\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manuel.barrera\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cae6ede5a394a1c97114f94896b17bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e5a4a20cef4ef9aef3515699f0f142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02292c9cd9ef4fb8a604c859247c6457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4616c96cfff425c84f7c4f9a65873b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc06233303d424fa79d47237e19e8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5446b5781def4181be61b0c73f06bdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello AI classroom, today we will learn about the power of AI to create a meaningful set of problems, and how it can help solve them.\\n\\nThe goal of this class is to introduce'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Clasificaci√≥n de sentimientos (modelo ligero por defecto)\n",
    "clf = pipeline(\"sentiment-analysis\")\n",
    "print(clf(\"Este curso de IA en el aula me parece excelente.\"))\n",
    "\n",
    "# Generaci√≥n de texto (modelo base peque√±o)\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=30)\n",
    "print(gen(\"Hello AI classroom, today we will learn about\", num_return_sequences=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87b99c24bf7482f86008231d60b1fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuel.barrera\\Downloads\\hello_ai_vscode\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manuel.barrera\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44acbcfd8b7c44e68f3ec81ac20250cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9f3462e2e146f9aae466d2bb459550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865b2895229a4227a06a183b8ffa787b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEGATIVE': 0.0004911543801426888, 'POSITIVE': 0.9995088577270508}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "inputs = tok(\"I love practical AI courses.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "pred = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "print({\"NEGATIVE\": pred[0], \"POSITIVE\": pred[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1803dc3f422b4cb2af588f77f72e658d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuel.barrera\\Downloads\\hello_ai_vscode\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manuel.barrera\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9aa99e050d47439ee33bb3918497fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087bc878447342fca4daf13dbfdb4125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733eb2b453054fa1ae24b5570634bf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990e2e3da4964f4d9dd671a456308e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556c24378bdb428e853a817b5bdbe03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 384])\n",
      "Cosine similarity (0 vs 1): 0.468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "emb_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "emb_tok = AutoTokenizer.from_pretrained(emb_model_id)\n",
    "emb_model = AutoModel.from_pretrained(emb_model_id)\n",
    "\n",
    "def embed(texts):\n",
    "    # Tokenizaci√≥n con padding/truncado\n",
    "    batch = emb_tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = emb_model(**batch)\n",
    "    # Mean pooling simple sobre la √∫ltima capa\n",
    "    tokens = out.last_hidden_state  # [batch, seq, hidden]\n",
    "    mask = batch[\"attention_mask\"].unsqueeze(-1)  # [batch, seq, 1]\n",
    "    masked = tokens * mask\n",
    "    sent_emb = masked.sum(dim=1) / mask.sum(dim=1)\n",
    "    return F.normalize(sent_emb, p=2, dim=1)\n",
    "\n",
    "e = embed([\"inteligencia artificial en educaci√≥n\", \"clase de programaci√≥n\", \"oxigenaci√≥n en hidroel√©ctricas\"])\n",
    "print(e.shape)\n",
    "# Similitud coseno entre primera y segunda\n",
    "sim = (e[0] @ e[1]).item()\n",
    "print(\"Cosine similarity (0 vs 1):\", round(sim, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer BPE de ejemplo creado (demo).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "# Ejemplo m√≠nimo: crear un tokenizer vac√≠o BPE (demostrativo)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "# Nota: entrenar un tokenizer real requiere un corpus y procesos adicionales (no cubierto aqu√≠).\n",
    "print(\"Tokenizer BPE de ejemplo creado (demo).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello_ai_vscode",
   "language": "python",
   "name": "hello_ai_vscode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
